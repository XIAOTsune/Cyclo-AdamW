# Cyclo-AdamW: Physics-Inspired Optimizer for Deep Learning <br> (åŸºäºç‰©ç†æ‘†çº¿åŸç†çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)

[English](#english) | [ä¸­æ–‡](#chinese)

---

<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English Description

**Cyclo-AdamW** is a novel optimization algorithm that bridges **Classical Mechanics** (Brachistochrone problem) and **Deep Learning**. By modeling the loss landscape as a gravitational potential field, it dynamically adjusts the learning rate and filters noise, achieving faster convergence and better generalization.

![Cycloid Animation](https://upload.wikimedia.org/wikipedia/commons/3/37/Brachistochrone.gif)
*(Concept: The Cycloid curve is the fastest path under gravity)*

### ğŸš€ Key Features

1.  **Cycloid Factor ($\phi$) with Energy Retention**:
    - Dynamically scales the step size based on **Potential Energy** (Loss).
    - **Energy Retention ($\gamma$)**: Allows tuning of how aggressively the learning rate decays as loss drops.
    - **Auto-Calibration**: Automatically resets the potential reference ($L_0$) if the loss landscape shifts significantly.

2.  **Quantum Threshold ($h_{DL}$)** via Mean Action Density:
    - Filters out "thermal noise" updates where the **Mean Action Density** (Average work per parameter) is below a threshold ($h_{DL}$).
    - **Scale Invariant**: Robust across different layer sizes (Conv2d vs Bias).
    - Stabilizes training in flat or noisy regions without killing effective gradients.

### ğŸ“Š Performance (Verified)

| Task | Metric | AdamW | Cyclo-AdamW | Improvement |
| :--- | :--- | :--- | :--- | :--- |
| **Non-Convex Opt** (Rosenbrock) | Final Loss | 3.9495 | **3.3123** | **-16% Loss** |
| **Image Classif** (MNIST) | Accuracy | 98.77% | **99.00%** | **+0.23% Acc** |
| **Complex Vision** (CIFAR-10) | Accuracy (100 Epochs) | **92.71%** | 92.61% | *Comparable (-0.1%)* |

> *See [Verification Report](docs/verification_report.md) for details.*

### ğŸ“¦ Installation

Copy the `src/cyclo_adamw.py` file to your project, or clone this repository:

```bash
git clone https://github.com/XIAOTsune/Cyclo-AdamW.git
cd Cyclo-AdamW
pip install -r requirements.txt
```

### ğŸ›  Usage

It functions as a drop-in replacement for `torch.optim.AdamW`.

```python
from src.cyclo_adamw import CycloAdamW

# Initialize Optimizer
optimizer = CycloAdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-2,
    h_dl=1e-8,          # Quantum Threshold (Default: 1e-8)
    gamma=0.25,         # Energy Retention (Default: 0.25)
    warmup_steps=500    # Warmup steps before physics logic activates
)
```

---

<a name="chinese"></a>
## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ä»‹ç»

**Cyclo-AdamW** æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•ï¼Œå®ƒå°† **ç»å…¸åŠ›å­¦**ï¼ˆæœ€é€Ÿé™çº¿é—®é¢˜ï¼‰ä¸ **æ·±åº¦å­¦ä¹ ** ç›¸ç»“åˆã€‚é€šè¿‡å°†æŸå¤±åœ°å½¢å»ºæ¨¡ä¸ºé‡åŠ›åŠ¿èƒ½åœºï¼Œå®ƒèƒ½å¤ŸåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡å¹¶è¿‡æ»¤å™ªå£°ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒç‰¹æ€§

1.  **å¸¦èƒ½é‡ä¿ç•™çš„æ‘†çº¿å› å­ (Cycloid Factor $\phi$)**:
    - æ ¹æ®ç›¸å¯¹äºåˆå§‹çŠ¶æ€çš„**åŠ¿èƒ½**ï¼ˆLossï¼‰åŠ¨æ€ç¼©æ”¾æ­¥é•¿ã€‚
    - **èƒ½é‡ä¿ç•™ ($\gamma$)**: å…è®¸è°ƒèŠ‚å­¦ä¹ ç‡éš Loss ä¸‹é™è€Œè¡°å‡çš„æ¿€è¿›ç¨‹åº¦ã€‚
    - **è‡ªåŠ¨æ ¡å‡†**: å¦‚æœ Loss åœ°å½¢å‘ç”Ÿå‰§çƒˆå˜åŒ–ï¼Œè‡ªåŠ¨é‡ç½®åŠ¿èƒ½å‚è€ƒç‚¹ ($L_0$)ã€‚

2.  **åŸºäºå¹³å‡ä½œç”¨é‡å¯†åº¦çš„é‡å­é˜ˆå€¼ ($h_{DL}$)**:
    - è¿‡æ»¤æ‰â€œçƒ­å™ªå£°â€æ›´æ–°ï¼Œå³å½“ **å¹³å‡ä½œç”¨é‡å¯†åº¦** (æ¯ä¸ªå‚æ•°çš„å¹³å‡åšåŠŸ) ä½äºé˜ˆå€¼ ($h_{DL}$) æ—¶ï¼ŒæŠ‘åˆ¶æ›´æ–°ã€‚
    - **å°ºåº¦ä¸å˜æ€§**: å¯¹ä¸åŒå¤§å°çš„å±‚ï¼ˆå¦‚å¤§å‹å·ç§¯å±‚ä¸å°å‹åç½®å±‚ï¼‰å…·æœ‰é²æ£’æ€§ã€‚
    - åœ¨å¹³å¦æˆ–å˜ˆæ‚åŒºåŸŸç¨³å®šè®­ç»ƒï¼ŒåŒæ—¶ä¿ç•™æœ‰æ•ˆæ¢¯åº¦ã€‚

### ğŸ“Š æ€§èƒ½è¡¨ç° (å·²éªŒè¯)

| ä»»åŠ¡ | æŒ‡æ ‡ | AdamW | Cyclo-AdamW | æå‡ |
| :--- | :--- | :--- | :--- | :--- |
| **éå‡¸ä¼˜åŒ–** (Rosenbrock) | æœ€ç»ˆ Loss | 3.9495 | **3.3123** | **Loss é™ä½ 16%** |
| **å›¾åƒåˆ†ç±»** (MNIST) | å‡†ç¡®ç‡ | 98.77% | **99.00%** | **å‡†ç¡®ç‡æå‡ 0.23%** |
| **å¤æ‚è§†è§‰** (CIFAR-10) | å‡†ç¡®ç‡ (3 Epochs) | **70.45%** | 68.40% | *ç›¸å½“ (-2%)* |

> *è¯¦è§ [éªŒè¯æŠ¥å‘Š](docs/verification_report.md)ã€‚*

### ğŸ“¦ å®‰è£…

å°† `src/cyclo_adamw.py` æ–‡ä»¶å¤åˆ¶åˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼Œæˆ–å…‹éš†æ­¤ä»“åº“ï¼š

```bash
git clone https://github.com/XIAOTsune/Cyclo-AdamW.git
cd Cyclo-AdamW
pip install -r requirements.txt
```

### ğŸ›  ä½¿ç”¨æ–¹æ³•

å®ƒå¯ä»¥ä½œä¸º `torch.optim.AdamW` çš„ç›´æ¥æ›¿ä»£å“ä½¿ç”¨ã€‚

```python
from src.cyclo_adamw import CycloAdamW

# åˆå§‹åŒ–ä¼˜åŒ–å™¨
optimizer = CycloAdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-2,
    h_dl=1e-8,          # é‡å­é˜ˆå€¼ (é»˜è®¤: 1e-8)
    gamma=0.25,         # èƒ½é‡ä¿ç•™å› å­ (é»˜è®¤: 0.25)
    warmup_steps=500    # ç‰©ç†é€»è¾‘æ¿€æ´»å‰çš„çƒ­å¯åŠ¨æ­¥æ•°
)
```

---

## ğŸ“‚ Project Structure / é¡¹ç›®ç»“æ„

```
Cyclo-AdamW/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ cyclo_adamw.py    # Core implementation / æ ¸å¿ƒå®ç°
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_convex.py    # Math function verification / æ•°å­¦å‡½æ•°éªŒè¯
â”‚   â”œâ”€â”€ test_mnist.py     # Deep learning verification / æ·±åº¦å­¦ä¹ éªŒè¯
â”‚   â””â”€â”€ test_cifar10.py   # Complex dataset verification / å¤æ‚æ•°æ®é›†éªŒè¯
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ algorithm_design.md  # Theory / ç†è®ºæ¨å¯¼
â”‚   â””â”€â”€ verification_report.md # Results / éªŒè¯æŠ¥å‘Š
â””â”€â”€ requirements.txt
```

## ğŸ“ Citation / å¼•ç”¨

If you use this optimizer in your research, please cite:
å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æ­¤ä¼˜åŒ–å™¨ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{CycloAdamW2026,
  author = {XIAOTsune},
  title = {Cyclo-AdamW: A Physics-Inspired Optimizer},
  year = {2026},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/XIAOTsune/Cyclo-AdamW}}
}
```

## ğŸ“„ License / è®¸å¯
This project is licensed under the **MIT License**.
æœ¬é¡¹ç›®é‡‡ç”¨ **MIT è®¸å¯è¯**ã€‚
